{
 "metadata": {
  "name": "",
  "signature": "sha256:8b648016bdbbfbff7957e9cb49e5042d3dea9026e9a0bd5cdff45387199e1072"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%html\n",
      "<link rel=\"stylesheet\" href=\"static/hyrule.css\" type=\"text/css\">"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<link rel=\"stylesheet\" href=\"static/hyrule.css\" type=\"text/css\">"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.HTML at 0x104396050>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "# note a change here\n",
      "# matplotlib is less specific and doesn't import anything \"extra.\"\n",
      "# compared to %pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# any other imports here\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from __future__ import division\n",
      "pd.set_option('display.max_rows', 500)\n",
      "pd.set_option('display.max_columns', 50)\n",
      "pd.set_option('display.width', 1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The Data Mining Workflow and SKLearn"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Objectives"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Understand the basics in a data pipeline model\n",
      "* Build helper functions to help standardize our processing with sklearn\n",
      "* Review the linear regression algorithm in detail\n",
      "* Explain challenges with linear regressions: multicollinearity, regularization techniques"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Code Dictionary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below's code are all parts of sklearn, so we'll refer to the function or class, the module (part) it comes from, and a short description.\n",
      "\n",
      "code         | module | description \n",
      "-------------|--------|------------\n",
      "`f_regression()` | `feature_selection` | calculates p-values and f-statistics given an X and y\n",
      "`train_test_split()` | `cross_validation` | splits arrays and data frames into two unique and random sets.\n",
      "`LinearRegression()` | `linear_model` | creates a linear regression object, which can then fit results and store model information\n",
      "`r2_score()` | `metrics` | specifically finds the $R^2$ value for a group of features and a y variable\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Notes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### How Sklearn works\n",
      "\n",
      "Scikit Learn (sklearn), a machine learning library for python, will do much of our heavy lifting. While sklearn has everything we've used in statsmodels, there are two distinct differences:\n",
      "\n",
      "1. sklearn is _modular_. We call sklearn modular because many of the classes (objects) we use can be easily switched out and replaced with other sklearn classes.\n",
      "2. sklearn's implementation feels more \"engineery.\" Instead of printing pretty tables, sklearn objects and their attributes return basic python and numpy data structures (floats/integers, lists, ndarrays, dictionaries)\n",
      "\n",
      "Therefore, while statsmodels is very functional as an exploratory tool (and for time series analysis!), sklearn can control our entire data mining workflow.\n",
      "\n",
      "### What is the Data Mining pipeline?\n",
      "\n",
      "So far in the coursework we've uncovered the heavylifting and processing of data, be it pulling datasets together, cleaning it up, aggregating and visualising. While we can use tools like regressions to fill in our exploratory view, our objectives are also often to predict and define. In order to do so, we need to:\n",
      "\n",
      "1. Understand the impact of our **model**. We define our model (for now) as the set of feature data ($X$) given a value or target to be predicted ($y$). In particular, we want a model that:\n",
      "    * Is logical (we should not see, for example, an inverse relationship if the value in someone's home and their income).\n",
      "    * Is statistically significant (low probabilities of significance due to randomness or chance)\n",
      "    * Has strong qualifiers or predictors in the outcome (if everything is significant, but the objective values for each feature are small, then our model does not best explain the predicted value).\n",
      "2. Evaluate the performance of our model by testing the predicted results\n",
      "3. Determine if there are additional needs to build a stronger model.\n",
      "\n",
      "Therefore, we need a solution to:\n",
      "\n",
      "1. Help us understand and automate the feature selection process. We can use the [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection) part of sklearn to do this.\n",
      "2. Split our data into a training set to \"learn\" from, and a test set to \"evaluate.\" We can use the [cross validation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation) module of sklearn for this part.\n",
      "3. Use an algorithm to learn and evaluate. For today, we'll stick with [generalized linear models](http://scikit-learn.org/stable/modules/linear_model.html#linear-model).\n",
      "4. Optimize to some performance metric. With linear models, we've been using r-squared ($R^2$), which is found in the [metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) module.\n",
      "\n",
      "Each class we use sklearn, we'll explore something new with feature selection or extraction, a cross validation technique, an algorithm, and review performance metrics.\n",
      "\n",
      "#### Practice: read through code and documentation\n",
      "\n",
      "On your own, read and comment on the following code, line by line, running it as necessary. Use the sklearn documentation or the built in help functions in ipython notebook to determine what's occurring. We'll review as a group."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# note: while it's considered common to import pandas as pd and matplotlib with plt,\n",
      "# i'm unaware of a \"best practice\" for sklearn modules.\n",
      "from sklearn import feature_selection as f_select\n",
      "from sklearn import linear_model as lm\n",
      "from sklearn import cross_validation as cv\n",
      "from sklearn import metrics\n",
      "from sklearn.datasets import load_boston\n",
      "\n",
      "boston = load_boston()\n",
      "desc = boston.DESCR\n",
      "bostondf = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
      "bostondf['MEDV'] = boston.target\n",
      "\n",
      "x_columns = list(bostondf.columns)\n",
      "y_column = 'MEDV'\n",
      "x_columns.remove(y_column)\n",
      "\n",
      "significant_columns = []\n",
      "pvals = []\n",
      "for feature in x_columns:\n",
      "    pval = f_select.f_regression(bostondf[[feature]], bostondf[y_column])\n",
      "    if pval[1][0] < 0.05:\n",
      "        significant_columns.append(feature)\n",
      "        pvals.append(pval[1][0])\n",
      "\n",
      "x_train, x_test, y_train, y_test = cv.train_test_split(bostondf[significant_columns],\n",
      "                                                           bostondf[y_column],\n",
      "                                                           test_size=0.333,\n",
      "                                                           random_state=1234)\n",
      "\n",
      "model = lm.LinearRegression().fit(x_train, y_train)\n",
      "\n",
      "print pd.DataFrame({\n",
      "    'column': significant_columns,\n",
      "    'coef': model.coef_,\n",
      "    'p-value': pvals,\n",
      "}).set_index('column')\n",
      "print\n",
      "print model.score(x_train, y_train)\n",
      "print metrics.r2_score(y_train, model.predict(x_train))\n",
      "print\n",
      "print model.score(x_test, y_test)\n",
      "print metrics.r2_score(y_test, model.predict(x_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "              coef       p-value\n",
        "column                          \n",
        "CRIM     -0.101202  2.083550e-19\n",
        "ZN        0.062922  5.713584e-17\n",
        "INDUS    -0.024451  4.900260e-31\n",
        "CHAS      2.743707  7.390623e-05\n",
        "NOX     -22.724112  7.065042e-24\n",
        "RM        2.415003  2.487229e-74\n",
        "AGE       0.004942  1.569982e-18\n",
        "DIS      -1.904339  1.206612e-08\n",
        "RAD       0.389172  5.465933e-19\n",
        "TAX      -0.014100  5.637734e-29\n",
        "PTRATIO  -1.118604  1.609509e-34\n",
        "B         0.007027  1.318113e-14\n",
        "LSTAT    -0.587270  5.081103e-88\n",
        "\n",
        "0.72839057184\n",
        "0.72839057184\n",
        "\n",
        "0.736360506263\n",
        "0.736360506263\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Discussion\n",
      "\n",
      "1. What seems to be the advantages to solving a prediction problem this way? What seem to be the disadvantages?\n",
      "2. What assumptions (statistically) are being made in the script above?\n",
      "3. How could be take the \"best of both worlds?\" (A being how we've approached data exploration, B being this approach)\n",
      "4. Given that, how do we now define our data mining approach?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Deep Dive: The data mining process\n",
      "\n",
      "#### Feature Selection based on $p$-values\n",
      "\n",
      "One stark difference between statsmodels and sklearn: the $p$-values from a linear regression don't exist at all on the linear regression object, but instead, are pulled from a function in the `feature_selection` module. As we discussed last week, $p$-values measure the probability that a relationship between two data sets is due to randomness. Therefore we want low $p$-values, or we want relationships that are likely _not_ due to randomness.\n",
      "\n",
      "$p$-values are functional for regressions. For other data mining problems, like classification, we'll use other statistical tests."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```python\n",
      "# The f_regression function returns both the f-statistic and p-values given a target variable\n",
      "# You can use a list of features, or just one.\n",
      "feature_selection.f_regression(df[list_of_features], df[target_variable])\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Model validation: The Random Test Train Split\n",
      "\n",
      "Typically our objectives for modeling are:\n",
      "\n",
      "* Train a model. This means we're building the explanation of the relationship between the X (independent variables) and y (target variable). In both statsmodels and sklearn, this happens when we call the `.fit()` function. \n",
      "* Test the results. We want to measure how effective the model can predict both data it has already seen, and data it has _not_ seen. This suggests how \"general\" the model is. If the model explains the training set very well, but the test set not well at all, we'd call this **overfitting**.\n",
      "* If the results are similar given some amount of error, we can then use the model to predict values for data we don't know the answer for.\n",
      "\n",
      "To do this correctly, we'll split our data set between a **training** set (to learn from) and a **test** set (to evaluate with).\n",
      "\n",
      "One way to create a training and test set is to randomly split the data up, given more data to train against, and a smaller set to test. In sklearn, we'll use the `test_train_split()` function.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src='img/holdout.png'>\n",
      "<small><em>http://scott.fortmann-roe.com/</em></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```python\n",
      "# given as many arrays as need to be split, it'll create a training set and a test set.\n",
      "# use test_size to define the percent of the data in the test set,\n",
      "# and assign a random number to random state. This ensures that everytime you run the split, it creates the same split.\n",
      "# This is an important step for allowing someone else to reproduce your work!\n",
      "train, test = cross_validation.train_test_split(df, test_size=float(), random_state=int())\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Metrics for Linear Regressions\n",
      "\n",
      "We've discussed how we would primarily use $R^2$, or the goodness of fit, as a metric for linear regressions. $R^2$ is best described as a method for the percent of data that can be explained by the model, where 1 would represent a perfect fit.\n",
      "\n",
      "While $R^2$ will always be (theoretically) between 0 and 1 independent of our data, we have other methods that will scale with the data:\n",
      "\n",
      "A: In theory, we'd minimize the sum of the squared residuals (RSS, or SSE).\n",
      "\n",
      "* $SST = \\Sigma(y_i-\\bar{y})^2$\n",
      "* $SSR = \\Sigma(\\hat{y_i}-\\bar{y})^2$\n",
      "* $SSE = \\Sigma(y_i-\\hat{y_i})^2$\n",
      "* $SST$ = Total Sum of Squared Deviations in $Y$ from its mean $y\u0302$ \n",
      "\n",
      "$SSR$ = Sum of squares due to regression\n",
      "\n",
      "$SSE$ = Sum of Squared Residiuals\n",
      "\n",
      "In some cases the total sum of squares equals the sum of the two other sums of squares defined above:\n",
      "\n",
      "$SST=SSR+SSE$\n",
      "\n",
      "And we can solve for $R^2$ using the formula $R^2 = SSR / SST$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum_squares_regr = np.sum((model.predict(x_train) - np.mean(y_train))**2)\n",
      "total_sum_squares = np.sum((y_train - np.mean(y_train))**2)\n",
      "r_squared = sum_squares_regr / total_sum_squares\n",
      "\n",
      "print sum_squares_regr\n",
      "print total_sum_squares\n",
      "print r_squared"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20130.3658381\n",
        "27636.7743027\n",
        "0.72839057184\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Deep Dive: What to pay attention to with linear regressions\n",
      "\n",
      "python code:\n",
      "\n",
      "```python\n",
      "model = linear_model.LinearRegression().fit(X, y)\n",
      "model.coef_\n",
      "model.y_intercept_\n",
      "model.score(X, y)\n",
      "```\n",
      "\n",
      "#### Single Variable Models\n",
      "\n",
      "Single variable = simplest. No completixities, see direction relationships and correlations. Our example from previous classes would be comparing animal brain weights to body weights.\n",
      "\n",
      "#### Multi Variable Models\n",
      "\n",
      "As you introduce more features into a model:\n",
      "\n",
      "1. \"best fit\" should improve: more data will generally do a better job predicting than less\n",
      "2. coefficients and pvalues change: if a newly introduced feature is more powerful and predictive, it could change the relationships from all other features. In worst cases, this could be a sign of **multicollinearity**.\n",
      "3. Features could be completely insignficant. Consider testing importance by comparing how $p$-values and $R^2$ changes on the overall effects of the model\n",
      "\n",
      "#### Testing for Multicollinearity\n",
      "\n",
      "Multicollinearity causes the linear regression model to break down, because it can\u2019t tell the predictor variables apart. This results in a singularity.\n",
      "\n",
      "How can we deal with multicolinearity? Replace the correlated predictors with uncorrelated predictors.\n",
      "\n",
      "#### Signs of Multicollinearity\n",
      "* A regression coefficient is not significant even though, theoretically, that variable should be highly correlated with Y.\n",
      "* When you add or delete an X variable, the regression coefficients change dramatically.\n",
      "* You see a negative regression coefficient when your response should increase along with X.\n",
      "* You see a positive regression coefficient when the response should decrease as X increases.\n",
      "* Your X variables have high pairwise correlations.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src='img/correlation_heatmap.png'>\n",
      "<small><em>http://www.biomedcentral.com</em></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Numpy has a [correlation matrix function](http://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html) that we can then plot the results using matplotlib's [pcolor](http://matplotlib.org/examples/pylab_examples/pcolor_demo.html) function."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "On Your Own"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Go back to the Boston housing dataset, as a whole, and:\n",
      "    * read the description to learn more about the features\n",
      "    * visualise the dataset, as we've practiced in class previously (how do we import matplotlib in ipython notebook?)\n",
      "2. Given the visualisations, what improvements can be made to the prepared model?\n",
      "3. The `f_regression()` function tested for single feature significance against the median value. Do we get the same results if we test multiple features at the same time? (try using multiple features with `f_regression()`).\n",
      "4. **\\*** Assume we wanted to run multiple test train splits (let's say 5), and take the average $R^2$. How could we do so using a for loop?\n",
      "5. **\\*** What if we wanted to run $k$ test train splits and take the average $R^2$, $k$ meaning however many times we wanted to loop. How could you wrap your answer for question (5) in a function do solve it?\n",
      "6. **\\*\\*** What is the most effective model you can create that best explains the median housing value?\n",
      "7. **\\*\\*\\*** Write a function that creates a similar summary table to that of statsmodels. You'll have to do some digging to find as much of the metric data as possible!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Review, Next Steps, Practice"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* To go much more in-depth on linear regression, read Chapter 3 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), from which this lesson was adapted. Alternatively, watch the [related videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/) or read Kevin Markham's (another GA instructor) [quick reference guide](http://www.dataschool.io/applying-and-interpreting-linear-regression/) to the key points in that chapter.\n",
      "* To review Statsmodels and gain a deeper interpretation of the various metrics, DataRobot has some decent posts:\n",
      "    * [simple linear regression](http://www.datarobot.com/blog/ordinary-least-squares-in-python/)\n",
      "    * [multiple linear regression](http://www.datarobot.com/blog/multiple-regression-using-statsmodels/)\n",
      "* This [introduction to linear regression](http://people.duke.edu/~rnau/regintro.htm) is much more detailed and mathematically thorough, and includes lots of good advice.\n",
      "* This is a relatively quick post on the [assumptions of linear regression](http://pareonline.net/getvn.asp?n=2&v=8)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}