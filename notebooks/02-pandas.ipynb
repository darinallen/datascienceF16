{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" href=\"static/hyrule.css\" type=\"text/css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Data with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Review different ways to pull data into pandas and the link between objects in python and pandas\n",
    "* Understand the differences between a DataFrame and a Series\n",
    "* Practice part of the ACES data exploration model\n",
    "* Learn imputation strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each class we'll make sure there's one location that shows any new functionality introduced, with explanations of how each method works.\n",
    "\n",
    "\n",
    "`pd.read_csv`: Reads in a csv file (by default) as a DataFrame. Does much more! check out [documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html)\n",
    "\n",
    "`pd.DataFrame`: Pandas' primary object, a 2-dimensional array (matrix).\n",
    "\n",
    "`pd.Series`: Pandas' other object, a 1-dimensional array (vector).\n",
    "\n",
    "functions | description\n",
    "----------|------------\n",
    "`.head()` | returns back the head of the pandas object (works for both DataFrame and Series)\n",
    "`.describe()` | returns statistical results back for the pandas object. \n",
    "`.isnull()` | returns back true for each value if it is null. Opposite of `.notnull()`\n",
    "`.dropna()` | returns back the object with the NA values dropped. `.dropna()` and the following four functions **do not** update the original pandas object. <br /> Can use the `inplace` argument to change that.\n",
    "`.ffill()` | front fills the missing data.\n",
    "`.bfill()` | back fills the missing data.\n",
    "`.fillna()` | fills missing data with whatever pass in as an argument. \n",
    "`.apply()` | apply a function to either a series (column) or DataFrame (by row)\n",
    "\n",
    "parameters | description\n",
    "-----------|------------\n",
    "`.columns` | returns back the columns\n",
    "`.index` | returns back the index of the object\n",
    "`[statement]` | commonly used for filtering to either a set of columns, rows, or data that is true to the boolean statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick introduction to iPython Notebook:\n",
    "\n",
    "Consider iPython notebook to be a great organization tool, but it takes much **practice** to keep it organized. Early recommendations while we practice using this tool in class:\n",
    "\n",
    "1. It is very easy to want to edit previous python cells. Until you're really confortatable with Notebook, please don't! Everything that you run stays **in the notebook**, so the moment you accidentally delete code that you might have been using, you lose your work the next time you run the notebook.\n",
    "2. The `mode` of Notebook is very similar to vim, so vim users should feel comfortable:\n",
    "    1. The natural mode is a command mode. If you press `esc`, you should be in this mode. It's for gettinng around, changing cell types, and other commands. While in command mode, if you press `h`, it'll let you know anything else you can do.\n",
    "    2. The other primary mode is edit mode. Pressing `return` on a cell will put you into edit mode (this would be similar to pressing `i` or `a` in vim). It's for editing cells, be they headers, markdown, code. We won't need nbconvert for the purposes of this course.\n",
    "3. Most developers initialize a notebook with customized pandas settings. Examples of those defaults are included below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "# pd.options.display.mpl_style = 'default'   # I got a warning that told me to use matplotlib.pyplot.style.use instead\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are customizable to your taste, and we will add more later. \n",
    "\n",
    "### Exploratory Data Analysis\n",
    "\n",
    "Primarily our goals for exploring data are the following:\n",
    "\n",
    "* **A**ssemble.\n",
    "* **C**lean.\n",
    "* **E**xplore.\n",
    "* **S**ummarize.\n",
    "\n",
    "Today we will review some common pandas commands (provided above) to focus on for this data exploration model.\n",
    "\n",
    "### Reading in Data\n",
    "\n",
    "Pandas will work across a variety of data inputs, including csv, excel, JSON, and using additional python libraries to connect to databases. For today, we'll focus on using the csv input. We'll use data about heart disease from the UC Irvine Machine Learning data repository[1]. \n",
    "\n",
    "Given this directory: http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/\n",
    "\n",
    "There are a few different data types, some processed, some not, and a \"names\" file. The names file will expose for us the columns for the processed data:\n",
    "```\n",
    "7. Attribute Information:\n",
    "-- Only 14 used\n",
    "  -- 1. #3  (age)       \n",
    "  -- 2. #4  (sex)       \n",
    "  -- 3. #9  (cp)        \n",
    "  -- 4. #10 (trestbps)  \n",
    "  -- 5. #12 (chol)      \n",
    "  -- 6. #16 (fbs)       \n",
    "  -- 7. #19 (restecg)   \n",
    "  -- 8. #32 (thalach)   \n",
    "  -- 9. #38 (exang)     \n",
    "  -- 10. #40 (oldpeak)   \n",
    "  -- 11. #41 (slope)     \n",
    "  -- 12. #44 (ca)        \n",
    "  -- 13. #51 (thal)      \n",
    "  -- 14. #58 (num)       (the predicted attribute)\n",
    "```\n",
    "\n",
    "Let's grab those fields as headers, and the processed cleveland data to work with in pandas (the .names file refers that this is the primary file used in research). Pandas io tools [2] handles http requests to grab files from the internet, though reminder that when doing so, it only saves the file in memory (in python), and not as a file on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header_row = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num',]\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'\n",
    "heart_data = pd.read_csv(url, header=0, na_values='?')\n",
    "heart_data.columns = header_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  slope   ca  thal  num\n",
       "0  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5    2.0  3.0   3.0    2\n",
       "1  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6    2.0  2.0   7.0    1\n",
       "2  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5    3.0  0.0   3.0    0\n",
       "3  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4    1.0  0.0   3.0    0\n",
       "4  56.0  1.0  2.0     120.0  236.0  0.0      0.0    178.0    0.0      0.8    1.0  0.0   3.0    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas is really just python\n",
    "Pandas is a library for python, built heavily around the task of manipulating and presenting data. If you're writing pandas code, you're writing python code! Pandas contains (primarily) two new python objects:\n",
    "\n",
    "* **DataFrame**: a wrapper around a 2 dimensional numpy ndarray (in math, we call this a matrix)\n",
    "* **Series**: a wrapper around a 1 dimensional numpy ndarray (in math, we call this a vector)\n",
    "\n",
    "### Math Jargon\n",
    "\n",
    "When working through data matrices and vectors, we'll also often use the words feature and observation. \n",
    "\n",
    "* **Feature**: a feature is represented by a column. It is a segmentation of your data. Features are usually either continuous values (representing -inf to inf, and 1 < 2) or categorical values (each value represents its own space; 1 !< 2).\n",
    "* **Observation**: an observation is a row of your data. It should represent a single entity (for example, a survey responder).\n",
    "* **Target Variable**: often we'll be working with a column called a target variable, or predicted value. In data analysis, it is often the goal to be able to statistically explain this variable using the observations and features.\n",
    "\n",
    "### DataFrames behave like lists\n",
    "DataFrames support many of the functionalities of lists, like slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Lists\n",
    "import random\n",
    "random_list = [random.random() for i in xrange(300)]\n",
    "\n",
    "#print random_list[3:14]\n",
    "#print random_list[280:]\n",
    "#print random_list[:20]\n",
    "\n",
    "#print len(random_list)\n",
    "\n",
    "## DataFrame\n",
    "#print heart_data[3:14]\n",
    "#print heart_data[280:]\n",
    "#print heart_data[:20]\n",
    "\n",
    "#print len(heart_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames also behave like dictionaries\n",
    "DataFrames support returning by column in a similar way a dictionary returns by key. Note that passing in a string for a key will return a pandas **Series**, while a list of keys will return a **DataFrame**.\n",
    "\n",
    "The following table of code shows similar code, dependent on your object type:\n",
    "\n",
    "action | dictionary | DataFrame\n",
    "-------|-----------|----------\n",
    "return values for a key | `some_dict['a']` | `heart_data['age']`\n",
    "creating a new key | `some_dict['d'] = [i**2 for i in some_dictionary['a']]` | `heart_data['random_var'] = random.random()`\n",
    "filtering results for a key | `[i for i in some_dict['a'] if i > .5]` | `heart_data[heart_data['random_var'] >.7]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [0.44235233535479723,\n",
       "  0.3176799458124815,\n",
       "  0.629975427455683,\n",
       "  0.5343683202766922,\n",
       "  0.8961034578338242,\n",
       "  0.34584682552503043,\n",
       "  0.19988794351356498,\n",
       "  0.6795922219772748,\n",
       "  0.12856546821481252,\n",
       "  0.18439355053861162,\n",
       "  0.666943177813894,\n",
       "  0.545423667339373,\n",
       "  0.8387262166620906,\n",
       "  0.30294891666195123,\n",
       "  0.8577616439326754,\n",
       "  0.38292062637909374,\n",
       "  0.1319908873313067,\n",
       "  0.46580569365386615,\n",
       "  0.1824422025324931,\n",
       "  0.15935693794613925],\n",
       " 'b': [0.815692625368358,\n",
       "  0.08387095999946903,\n",
       "  0.9745999772401444,\n",
       "  0.9071604228869058,\n",
       "  0.7294165031723566,\n",
       "  0.031214808838998298,\n",
       "  0.4991275891318123,\n",
       "  0.9104282436501846,\n",
       "  0.7454717720348414,\n",
       "  0.58619136029403,\n",
       "  0.9731586173535935,\n",
       "  0.2731465211546823,\n",
       "  0.5377127939155323,\n",
       "  0.09645281801063621,\n",
       "  0.9243175883533098,\n",
       "  0.9314401835691123,\n",
       "  0.8346449246237398,\n",
       "  0.8351825274833256,\n",
       "  0.8624686176622739,\n",
       "  0.7727622613448208],\n",
       " 'c': [0.29543924634748875,\n",
       "  0.9033372848453963,\n",
       "  0.9612379301690384,\n",
       "  0.16786462761733156,\n",
       "  0.24432930297247268,\n",
       "  0.21508114964457148,\n",
       "  0.8778678828339326,\n",
       "  0.7082294483080342,\n",
       "  0.8623226037980084,\n",
       "  0.6795993616242557,\n",
       "  0.8144756169809428,\n",
       "  0.4641985711673029,\n",
       "  0.8933909031850719,\n",
       "  0.5948249807296471,\n",
       "  0.4062213138425129,\n",
       "  0.02950112463507426,\n",
       "  0.2876564921021675,\n",
       "  0.38813578056685294,\n",
       "  0.6397646860094182,\n",
       "  0.6250572059853517],\n",
       " 'd': [0.195675588593843,\n",
       "  0.10092054797142118,\n",
       "  0.39686903919797056,\n",
       "  0.28554950171533355,\n",
       "  0.8030014071417363,\n",
       "  0.11961002672574084,\n",
       "  0.039955189962082145,\n",
       "  0.4618455881720095,\n",
       "  0.016529079617293968,\n",
       "  0.03400098148023552,\n",
       "  0.44481320243249545,\n",
       "  0.297486976893931,\n",
       "  0.7034616665163042,\n",
       "  0.09177804610664987,\n",
       "  0.7357550378020858,\n",
       "  0.1466282061065575,\n",
       "  0.0174215943385057,\n",
       "  0.2169749442403594,\n",
       "  0.03328515726490723,\n",
       "  0.025394633671569678]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dictionary\n",
    "some_dict = {k: [random.random() for i in xrange(20)] for k in ['a', 'b', 'c',]}\n",
    "some_dict['d'] = [i**2 for i in some_dict['a']]\n",
    "some_dict\n",
    "#print some_dict['a']\n",
    "#print some_dict['b']\n",
    "#print [i for i in some_dict['a'] if i > .5]\n",
    "\n",
    "## DataFrame\n",
    "#heart_data['random_var'] = [random.random() for i in heart_data.index]  # add a column called \"random_var\"\n",
    "#print(heart_data[['num','random_var']])    # create a subset of just columns num and random_var\n",
    "\n",
    "#print heart_data['age'].head()\n",
    "#print heart_data['sex'].head()\n",
    "#print heart_data[['cp', 'oldpeak']].head()\n",
    "\n",
    "# Note on Printing Types -- What do we get back?\n",
    "#print type(heart_data['sex'])\n",
    "#print type(heart_data[['sex']])\n",
    "#print type(heart_data[['cp', 'oldpeak']])\n",
    "\n",
    "# Creating a subset:\n",
    "# Since random.random uses a flat distribution, consider the below as one approach to pick a random subset.\n",
    "#heart_data['random_var'] >.7\n",
    "#heart_subset = heart_data[heart_data['random_var'] >.7].head()     #return True items\n",
    "#heart_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice: Selecting and subsetting data\n",
    "\n",
    "Let's write code to do the following things:\n",
    "\n",
    "1. Return back the `head()` of the `chol` column\n",
    "2. Return back all rows where `thalach` is below 150.\n",
    "3. Return back all rows where `age` is above the median (hint, you can get the median of a column using `df.col.median()`)\n",
    "4. Summarise the resting heart beat rate when the pain type is asymptomatic (look up columns in the text file, and use `.describe()`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1 heart_data[\"chol\"].head()\n",
    "#2 heart_data[heart_data[\"thalach\"] < 150]\n",
    "#3 heart_data[heart_data[\"age\"] > heart_data.age.median()]\n",
    "#4 heart_data[heart_data[\"cp\"] == 4].describe()[[\"trestbps\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "\n",
    "Our primary task for today to clean is to find and handle missing values. Data can be missing for different reasons:\n",
    "\n",
    "* There was no response value. This is common in True/False data, where True could be a yes, False, could be a no, and NA just means there was no answer.\n",
    "* The data was poorly handled. Missing data happens all the time on poor data imports.\n",
    "* The missing data really should have just been a 0.\n",
    "\n",
    "To find missing data, we can use pandas `.describe()` function, which uses `count` as a \"count of non null values\" field, and the `.isnull()` function once we've identified rows missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Looks like we have 4 missing values in the ca column, or the number of primary vessels, so let's focus on that one first.\n",
    "#print heart_data[heart_data['ca'].isnull()]\n",
    "\n",
    "heart_data[heart_data.isnull().any(axis=1)]    # Rows for which data is null in any column (i.e. axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose the handle the data a few different ways. This handling is called data imputation.\n",
    "\n",
    "1. Remove the data. This makes sense if we think it's bad data.\n",
    "2. Fill the data. Common techniques would be to fill with some default value (0), or backfill/forwardfill the data, based on the sort.\n",
    "3. interpolate the data. This technique is an estimation, sometimes with machine learning techniques. More on this later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Trial 1: Drop missing values. \n",
    "Pandas defines this on the axis (think axis of a matrix):\n",
    "0: along the columns\n",
    "1: along the rows\n",
    "\"\"\"\n",
    "#print heart_data[285:290]\n",
    "\n",
    "#dropped_rows_heart_data = heart_data.dropna()\n",
    "#dropped_ca_thal_heart_data = heart_data.dropna(1)\n",
    "\n",
    "#print dropped_rows_heart_data[285:290]\n",
    "#print dropped_ca_thal_heart_data[285:290]\n",
    "\n",
    "\"\"\" Trial 2: Fill the data\n",
    ".ffill(): fills forward\n",
    ".bfill(): fills backward\n",
    ".fillna(): fills based on some value\n",
    "\"\"\"\n",
    "#print heart_data[285:290]['ca'].ffill()\n",
    "#print heart_data[285:290]['ca'].bfill()\n",
    "#print heart_data[285:290]['ca'].fillna(0) # fill with 0s\n",
    "#print heart_data[285:290]['ca'].fillna(int(heart_data['ca'].mode())) # fill with the most common value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Together: Filling Missing Data\n",
    "\n",
    "One other column was missing data in the heart disease data set.\n",
    "\n",
    "1. You can find it if you look at your counts in .describe. What was it?\n",
    "2. Look up the information about that column in the names file. What is that column about?\n",
    "3. Determine three different ways to fill that column. Which ways seems most ideal for this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# heart_data.describe()\n",
    "#1 \"thal\"\n",
    "#2 \"If the status is normal (3), has a fixed defect (6), or a reversible defect (7)\"\n",
    "\n",
    "#heart_data\n",
    "#heart_data.dropna(subset=[\"thal\"])    #drops all rows that have NaN in the \"thal\" column\n",
    "#heart_data[heart_data[\"thal\"].isnull()]\n",
    "\n",
    "# print heart_data['thal'].ffill()\n",
    "# print heart_data['thal'].bfill()\n",
    "# print heart_data['thal'].fillna(0) # fill with 0s\n",
    "# print heart_data['thal'].fillna(int(heart_data['thal'].mode())) # fill with the most common value\n",
    "# I would think interpolating with the most common value would work best here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One commont technique to data manipulation is to generate new data based on data already in the DataFrame. Pandas uses a function called `.apply()` in order to run such functions, either named functions (`def`) or nameless functions (`lambda`). `apply()` is particularly helpful for iterating through pandas data.\n",
    "\n",
    "Try it out by making a function where the patients resting heart beat (trestbps) was higher than their maximum heart rate achieved (thalach)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def high_resting(row):\n",
    "    if row['trestbps'] > row['thalach']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# axis = 1 means what again?\n",
    "heart_data['high_resting'] = heart_data.apply(high_resting, axis=1)\n",
    "print(heart_data['high_resting'])\n",
    "\n",
    "# This would work the same way!\n",
    "heart_data['high_resting'] = heart_data.apply(lambda row: 1 if row['trestbps'] > row['thalach'] else 0, axis=1)\n",
    "\n",
    "# axis = 0 means what again?\n",
    "heart_data['resting2x'] = heart_data['trestbps'].apply(lambda x: x*2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Your Own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're going work on similar questions with another data set from UC Irvine on vehicle mileage per gallon. You can find it with this link:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/\n",
    "\n",
    "1. Bring the data into a DataFrame with pandas. The file is `auto-mpg.data`. Since it is space seperated, you'll have to tell read_csv to use spaces (`'\\s+'`) and not commas (`','`) as the delimeter. Likewise, you have to name the columns.\n",
    "2. Compare the data for cars from the year 1970 and the year 1982. In general in this data set, have cars changed in terms of mpg, horsepower, etc? (any of the continuous values)\n",
    "3. Horsepower is missing several values. What are some basic techniques to fill in the missing data?\n",
    "\n",
    "** more advanced **\n",
    "\n",
    "4. Consider splitting the data by year and filling in horsepower that way. What would the python code look like to hande this?\n",
    "5. A handy way to work with strings is to `.split()` them by a value into a python list. Write an `apply()` function that generates a new column called \"manufacturer,\" using the last column's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1. Bring the data into a DataFrame with pandas. The file is \n",
    "auto-mpg.data. Since it is space seperated, you'll have \n",
    "to tell read_csv to use spaces ('\\s+') and not commas (',') \n",
    "as the delimeter. Likewise, you have to name the columns.\n",
    "'''\n",
    "auto_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
    "auto_header_row = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin', 'car name',]\n",
    "auto_data = pd.read_csv(auto_url, sep='\\s+', header=0, na_values='?')\n",
    "auto_data.columns = auto_header_row\n",
    "auto_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "2. Compare the data for cars from the year 1970 and the year \n",
    "1982. In general in this data set, have cars changed in \n",
    "terms of mpg, horsepower, etc? (any of the continuous values)\n",
    "'''\n",
    "\n",
    "'''\n",
    "Darin Notes:\n",
    "Yes, horsepower and cylinders seem to have gone down on average,\n",
    "and mpg has gone up.\n",
    "'''\n",
    "auto_data[(auto_data[\"model year\"] == 70 ) | (auto_data[\"model year\"] == 82)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Horsepower is missing several values. What are some basic \n",
    "techniques to fill in the missing data?\n",
    "'''\n",
    "\n",
    "#  print auto_data['horsepower'].ffill()\n",
    "#  print auto_data['horsepower'].bfill()\n",
    "#  print auto_data['horsepower'].fillna(0) # fill with 0s\n",
    "#  print auto_data['horsepower'].fillna(int(auto_data['horsepower'].mode())) # fill with the most common value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Consider splitting the data by year and filling in horsepower that way. \n",
    "What would the python code look like to hande this?\n",
    "'''\n",
    "\n",
    "auto_years = auto_data.groupby(\"model year\")\n",
    "auto_years.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lst = [1, 2, 3, 1, 2, 3]\n",
    "\n",
    "#s = pd.Series([1, 2, 3, 10, 20, 30], lst)\n",
    "\n",
    "#print s\n",
    "\n",
    "#grouped = s.groupby(level=0)\n",
    "\n",
    "#print grouped.first()\n",
    "\n",
    "#print grouped.last()\n",
    "\n",
    "#grouped.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A handy way to work with strings is to .split() them by a value into a python list. \n",
    "Write an apply() function that generates a new column called \"manufacturer,\" using the last column's data.\n",
    "'''\n",
    "auto_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review, Next Steps, Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For next class:\n",
    "\n",
    "0. On our off day, please review this whole notebook again, and make sure you can follow along without guidance! Use slack when you have questions. You should do this with each class notebook.\n",
    "1. Read through the documentation for [split, apply, combine](http://pandas.pydata.org/pandas-docs/stable/groupby.html). It's a technique we will be using in more detail with the next few classes.\n",
    "2. To understand the choices in data storage, read about [tidy data](http://vita.had.co.nz/papers/tidy-data.pdf). \n",
    "3. Additional resource for [tidy data](http://www.prometheusresearch.com/good-data-management-practices-for-data-analysis-tidy-data-part-2/).\n",
    "4. The two visualisation packages we will be using are [matplotlib](http://matplotlib.org/) and [seaborn](http://stanford.edu/~mwaskom/software/seaborn/). Consider trying out some sample code. You'll need to install seaborn with `conda`. We'll do that as a class on Wednesday.\n",
    "5. Questions to think about when doing [Exploratory Data Analysis](http://www.itl.nist.gov/div898/handbook/eda/section3/eda32.htm)\n",
    "\n",
    "###### Resource links from today\n",
    "1. [UC Irvine Data Repository](http://archive.ics.uci.edu/ml/)\n",
    "2. [Pandas IO Cookbook](http://pandas.pydata.org/pandas-docs/stable/cookbook.html#data-in-out)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
